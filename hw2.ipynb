{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5864fbe4",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "de5951a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from random import sample\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "df = pd.DataFrame(data)\n",
    "df = df.drop(['id'],axis =1)\n",
    "train_data = np.array(df.drop(['diagnosis'],axis =1))\n",
    "train_data = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(train_data)\n",
    "train_data = StandardScaler().fit_transform(train_data)\n",
    "test = np.array(df['diagnosis'])\n",
    "\n",
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1)-1):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    return sqrt(distance)\n",
    "        \n",
    "\n",
    "def k_means(data,k,itera,centroids):\n",
    "    \n",
    "    i,j = sample(range(len(data)),2)\n",
    "    centroids[0] = data[i]\n",
    "    centroids[1] = data[j]\n",
    "    \n",
    "    for i in range(itera):\n",
    "        classifications = {}\n",
    "    \n",
    "        for i in range(k):\n",
    "            classifications[i] = []\n",
    "\n",
    "        for row in data:\n",
    "            distance0 = euclidean_distance(row,centroids[0])\n",
    "            distance1 = euclidean_distance(row,centroids[1])\n",
    "            if(distance0<distance1):\n",
    "                classifications[0].append(row)\n",
    "            else:\n",
    "                classifications[1].append(row)\n",
    "        \n",
    "        prev_centroids = dict(centroids)\n",
    "        \n",
    "        for classification in classifications:\n",
    "                centroids[classification] = np.average(classifications[classification],axis=0)\n",
    "       \n",
    "        flag = False\n",
    "        for c in centroids:\n",
    "                original_centroid = prev_centroids[c]\n",
    "                current_centroid = centroids[c]\n",
    "                if(original_centroid.all() == current_centroid.all()):\n",
    "                    flag = True\n",
    "        if flag:\n",
    "            break\n",
    "\n",
    "def predict(row,centroids):\n",
    "    distance0 = euclidean_distance(row,centroids[0])\n",
    "    distance1 = euclidean_distance(row,centroids[1])\n",
    "    if(distance0<distance1):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "0b2bec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = {}\n",
    "k_means(train_data,2,200,centroids)\n",
    "correct1 = 0\n",
    "correct2 = 0\n",
    "test_trans1 = []\n",
    "test_trans2 = []\n",
    "for i in range(len(test)):\n",
    "    if (test[i] == 'M'):\n",
    "        test_trans1.append(0)\n",
    "        test_trans2.append(1)\n",
    "    elif (test[i] == 'B'):\n",
    "        test_trans1.append(1)\n",
    "        test_trans2.append(0)\n",
    "        \n",
    "for i in range(len(train_data)):\n",
    "\n",
    "    predict_me = train_data[i]\n",
    "    prediction = predict(predict_me,centroids)\n",
    "    #print(prediction,test[i])\n",
    "    if prediction == test_trans1[i]:\n",
    "        correct1 += 1\n",
    "    elif prediction == test_trans2[i]:\n",
    "        correct2 += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052916ea",
   "metadata": {},
   "source": [
    "Note: Since the cluster 0 can be either 'B' or 'M' checking both the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "5ba5da2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of correct prediction 91.3884007029877\n",
      "false prediction 8.611599297012303\n"
     ]
    }
   ],
   "source": [
    "if(correct1>correct2):\n",
    "    print(\"accuracy of correct prediction\",correct1/len(train_data)*100)\n",
    "    print(\"false prediction\",correct2/len(train_data)*100)\n",
    "else:\n",
    "    print(\"accuracy of correct prediction\",correct2/len(train_data)*100)\n",
    "    print(\"false prediction\",correct1/len(train_data)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2e4489",
   "metadata": {},
   "source": [
    "As we are choosing centroid randomly  the accuracy varies depending on the centroids choosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "8d86b508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, test, random_state=0)\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "print(\"Test score: {:.2f}\".format(logreg.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac2bff",
   "metadata": {},
   "source": [
    "A supervised algorithm predicts more accurately than our model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233cda32",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8961a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "boys = pd.read_csv(\"boy_names.csv\")\n",
    "girls = pd.read_csv(\"girl_names.csv\")\n",
    "test = pd.read_csv(\"test_names.csv\")\n",
    "\n",
    "boys = pd.DataFrame(boys)\n",
    "girls = pd.DataFrame(girls)\n",
    "test = pd.DataFrame(test)\n",
    "\n",
    "bnames = np.array(boys['x'])\n",
    "gnames = np.array(girls['x'])\n",
    "testnames = np.array(test['x'])\n",
    "\n",
    "bdict = []\n",
    "f_countb = 0\n",
    "for i in range(len(bnames)-1):\n",
    "    features = nltk.ngrams(bnames[i], 3)\n",
    "    features = list(features)\n",
    "    bdict.append(features)\n",
    "    f_countb+= len(features)\n",
    "\n",
    "gdict = []\n",
    "f_countg = 0\n",
    "for i in range(len(gnames)-1):\n",
    "    features = nltk.ngrams(gnames[i], 3)\n",
    "    features = list(features)\n",
    "    gdict.append(features)\n",
    "    f_countg+= len(features)\n",
    "\n",
    "\n",
    "def nb_classifier(name):\n",
    "    \n",
    "    features = nltk.ngrams(name, 3)\n",
    "    features = list(features)\n",
    "    prob_mlb = 1\n",
    "    prob_mlg = 1\n",
    "    for f in features:\n",
    "        count = 1\n",
    "        for i in range(len(bdict)-1):\n",
    "            if f in bdict[i]:\n",
    "                count+= 1\n",
    "        #Computing P(X/Y)        \n",
    "        prob_mlb *= count/(f_countb+f_countg)\n",
    "        \n",
    "        count = 1\n",
    "        for i in range(len(gdict)-1):\n",
    "            if f in gdict[i]:\n",
    "                count+= 1\n",
    "        #Computing P(X/Y) \n",
    "        prob_mlg *= count/(f_countb+f_countg)\n",
    "        \n",
    "    #Computing P(Y)\n",
    "    prob_bf = f_countb/(f_countb+f_countg)\n",
    "    prob_gf = f_countg/(f_countb+f_countg)\n",
    "    \n",
    "    #Computing log P(Y=1/X)/P(Y=-1/X)\n",
    "    logp = math.log((prob_mlg*prob_gf)/(prob_mlb*prob_bf))\n",
    "    \n",
    "    # P(Y=1/X) > P(Y=-1/X) so returning +1 for girl\n",
    "    if(logp > 0):\n",
    "        return \"+1\"\n",
    "    # P(Y=1/X) < P(Y=-1/X) so returning -1 for boy\n",
    "    elif(logp < 0):\n",
    "        return \"-1\"\n",
    "    else:\n",
    "        return \"0\"\n",
    "\n",
    "    \n",
    "def predict(names):\n",
    "    dct = {'x':names,'classification':[]}\n",
    "    count = 0\n",
    "    count1 = 0\n",
    "    for i in names:\n",
    "        dct['classification'].append(nb_classifier(i))\n",
    "    return dct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3523e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           x classification\n",
      "0   Brittani             +1\n",
      "1    Brandin             -1\n",
      "2      Darry             -1\n",
      "3      Tresa             -1\n",
      "4    Fabiola             +1\n",
      "..       ...            ...\n",
      "95  Migdalia             +1\n",
      "96     Abril             +1\n",
      "97    Aliyah             +1\n",
      "98    Tianna             +1\n",
      "99     Colie             -1\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = predict(testnames)\n",
    "df = pd.DataFrame(data, columns= ['x', 'classification'])\n",
    "df.to_csv('results.csv')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85509d5f",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c527e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = lambda x,y: (x-2)**2 + (y-3)**2\n",
    "f2 = lambda x,y: (1-(y-3))**2 + 20*(((x+3)-(y-3)**2)**2)\n",
    "\n",
    "d1x = lambda x,y: 2*(x-2)\n",
    "d1y = lambda x,y: 2*(y-3)\n",
    "\n",
    "d2x = lambda x,y: 40*(x+3) - 40*((y-3)**2)\n",
    "d2y = lambda x,y: 2*(y-4) + 80*((y-3)**3) - 80*(x+3)*(y-3)\n",
    "\n",
    "\n",
    "def grad_descent(func,dx,dy,lr,itera,threshold):\n",
    "    x=0\n",
    "    y=0\n",
    "    fhist = []\n",
    "    fhist.append(func(x,y))   \n",
    "    for i in range(itera):\n",
    "        newx = x - lr*dx(x,y)\n",
    "        newy = y - lr*dy(x,y)\n",
    "        x = newx\n",
    "        y = newy\n",
    "        fhist.append(func(x,y))\n",
    "        if abs(fhist[-1])<threshold:\n",
    "            print(\"Found optimal solution x,y\" )\n",
    "            print(x,y)\n",
    "            return fhist\n",
    "        if fhist[-2]*1000<fhist[-1]:\n",
    "            print(\"Diverged too far ending\")\n",
    "            return fhist\n",
    "    print(\"Solution after max iterations\")\n",
    "    return fhist\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8a8ea613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found optimal solution x,y\n",
      "2.0 3.0\n",
      "0.0\n",
      "Diverged too far ending\n",
      "5402136508880.0\n",
      "Solution after max iterations\n",
      "0.0805020538826743\n"
     ]
    }
   ],
   "source": [
    "g1 = grad_descent(f1,d1x,d1y,0.5,10,0.0000000001)\n",
    "print(g1[-1])\n",
    "\n",
    "g2 = grad_descent(f2,d2x,d2y,0.5,100,0.0000000001)\n",
    "print(g2[-1])\n",
    "\n",
    "\n",
    "#0.00210985161208 learning rate by trail and error\n",
    "g2t = grad_descent(f2,d2x,d2y,0.00210985,100,0.0000000001)\n",
    "print(g2t[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "91f78d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1x2 = 2\n",
    "d1y2 = lambda x,y: 2\n",
    "d2x2 = 40\n",
    "d2y2 = lambda x,y: 2 + 240*((y-3)**2) - 80*(x+3)\n",
    "\n",
    "\n",
    "def newton_method(func,dx,dy,dx2,dy2,x0,y0,itera,epsilon,gamma = 1):\n",
    "    x = x0\n",
    "    y = y0\n",
    "    fhist = []\n",
    "    fhist.append(func(x,y))    \n",
    "    for i in range(itera):\n",
    "        if abs(fhist[-1])<epsilon:\n",
    "                print(\"Optimal solution found\")\n",
    "                print(x,y)\n",
    "                return fhist\n",
    "        if(dx2 != 0 and dy2(x,y) != 0):\n",
    "            newx = x - gamma*(dx(x,y)/dx2)\n",
    "            newy = y - gamma*(dy(x,y)/dy2(x,y))\n",
    "            x = newx\n",
    "            y = newy\n",
    "            fhist.append(func(x,y))\n",
    "    print(\"After max iterations\")\n",
    "    return fhist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e2425f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution found\n",
      "2.0 3.0\n",
      "0.0\n",
      "After max iterations\n",
      "12.04532252366209\n"
     ]
    }
   ],
   "source": [
    "n1 = newton_method(f1,d1x,d1y,d1x2,d1y2,0,0,10,0.0000000001)\n",
    "print(n1[-1])\n",
    "\n",
    "n2 = newton_method(f2,d2x,d2y,d2x2,d2y2,0,0,100,0.0000000001,0.1)\n",
    "print(n2[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419fe0c",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c93a989",
   "metadata": {},
   "source": [
    "1. The equality doesnt hold when the features are dependent on each other the left side value will be larger than right side if any of the feature has zero probability consider the example of spam filtering if the spam has any new word that has been not classified as spam\n",
    "    The right side value will be greater if the joint probability of features is less than individual features we can take the same example of spam filtering if car and medicine appeared in an email the joint probability of car and medicine in a spam email is very low but if we take individual probabilities the word car in spam mail can be high and the word medicine is also high in this case the left side dominates\n",
    "\n",
    "2. P(x/y=c) = P(x,y=c)/P(y=c)\n",
    "   P(x,y) can be further expanded by chain rule\n",
    "   P(x1,x2,x3.....xD,y=c) = P(xd/xd-1,.....x3,x2,x1,y=c)*P(xd-1,.....x3,x2,x1,y=c)\n",
    "                            P(xd/xd-1,.....x3,x2,x1,y=c)*P(xd-1/xd-2.....x3,x1,y=c)*P(xd-2.....x2,x1,y=c)\n",
    "                            P(xd/xd-1,.....x3,x2,x1,y=c)*P(xd-1/xd-2.....x3,x1,y=c).......*P(x1/y=c)*P(y=c)\n",
    "3. For the fixed features D by our assumption of independent features in naive bayes we can compute all the the probabilities easily with less training data and predict accuarately where as the full model needs lots of training data to compute all the joint probabilities and doesnt perform very well with less training data. So naive bayes gives the lower test set error\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379c58c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
